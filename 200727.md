Day 00.

# DevOps를 위한 Kubernetes 컨테이너 클러스터 심화 과정


## 목차


## 1. 스토리지

> ### 1.1 EmptyDir 볼륨

## 2. 쿠버네티스 기본 

> ### 2.1 구성요소 및 API
------------
 
## 1. 스토리지


### 1.1 EmptyDir 볼륨


#### 

> * fortune.sh
```
# vi fortune.sh

#!/bin/bash
trap "exit" SIGINT
mkdir /var/htdocs
while:
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep 3
 done
```

> * Dockerfile
```
# vi Dockerfile

From ubuntu:latest

RUN apt-get update ; apt-get -y install fortune
ADD fortune.sh /bin/fortune.sh

ENTRYPOINT /bin/fortune.sh
```

> * docker image build --network host -t myweb .


> * mynapp-rs-emptydir.yml
```
# vi mynapp-rs-emptydir.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-fortune
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mynapp-rs-fortune
  template:
    metadata:
      labels:
        app: mynapp-rs-fortune
    spec:
      containers:
      - name: web-server
        image: nginx:alpine
        volumeMounts:
        - name: web-fortune
          mountPath: /usr/share/nginx/html
          readOnly: true
        ports:
        - containerPort: 80
      - name: html-generator
        image: union1991/fortune
        volumeMounts:
        - name: web-fortune
          mountPath: /var/htdocs
      volumes:
      - name: web-fortune
        emptyDir: {}


```

> * mynapp-svc-emptydir.yml
```
# vi mynapp-svc-emptydir.yml

apiVersion: v1
kind: Service
metadata:
  name: mynapp-svc-fortune
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: mynapp-rs-fortune
```
> *  kubectl create -f mynapp-rs-emptydir.yml -f mynapp-svc-emptydir.yml 


> * kubectl exec mynapp-rs-fortune-6k4z8 -c web-server -- cat /usr/share/nginx/html/index.html




> * mynapp-pod-git.yml 생성
```
# vi mynapp-pod-git.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-git
spec:
  initContainers:
  - name: git-clone
    image: alpine/git
    args:
    - clone
    - --single-branch
    - --
    - https://github.com/kubernetes/kubernetes
    - /repo
    volumeMounts:
    - name: git-repository
      mountPath: /repo
  containers:
  - name: git-container
    image: busybox
    args: ['tail', '-f', '/dev/null']
    volumeMounts:
    - name: git-repository
      mountPath: /repo
  volumes:
  - name: git-repository
    emptyDir: {}

```
> * pod 생성 확인
```
# kubectl get po --watch
NAME                      READY   STATUS     RESTARTS   AGE
mynapp-pod-git            0/1     Init:0/1   0          45s
# kubectl exec mynapp-pod-git -- ls -l /repo
total 204
lrwxrwxrwx    1 root     root            21 Jul 27 02:35 BUILD.bazel -> build/root/BUILD.root
drwxr-xr-x    2 root     root          4096 Jul 27 02:35 CHANGELOG
lrwxrwxrwx    1 root     root            19 Jul 27 02:35 CHANGELOG.md -> CHANGELOG/README.md
-rw-r--r--    1 root     root           624 Jul 27 02:35 CONTRIBUTING.md
-rw-r--r--    1 root     root         11358 Jul 27 02:35 LICENSE
drwxr-xr-x    3 root     root          4096 Jul 27 02:35 LICENSES
lrwxrwxrwx    1 root     root            19 Jul 27 02:35 Makefile -> build/root/Makefile
lrwxrwxrwx    1 root     root            35 Jul 27 02:35 Makefile.generated_files -> build/root/Makefile.generated_files
-rw-r--r--    1 root     root           783 Jul 27 02:35 OWNERS
-rw-r--r--    1 root     root          9714 Jul 27 02:35 OWNERS_ALIASES
-rw-r--r--    1 root     root          3468 Jul 27 02:35 README.md
-rw-r--r--    1 root     root           563 Jul 27 02:35 SECURITY_CONTACTS
-rw-r--r--    1 root     root          1110 Jul 27 02:35 SUPPORT.md
lrwxrwxrwx    1 root     root            20 Jul 27 02:35 WORKSPACE -> build/root/WORKSPACE
drwxr-xr-x    4 root     root          4096 Jul 27 02:35 api
drwxr-xr-x   11 root     root          4096 Jul 27 02:35 build
drwxr-xr-x    9 root     root          4096 Jul 27 02:35 cluster
drwxr-xr-x   23 root     root          4096 Jul 27 02:35 cmd
-rw-r--r--    1 root     root           148 Jul 27 02:35 code-of-conduct.md
drwxr-xr-x    2 root     root          4096 Jul 27 02:35 docs
-rw-r--r--    1 root     root         33486 Jul 27 02:35 go.mod
-rw-r--r--    1 root     root         56100 Jul 27 02:35 go.sum
drwxr-xr-x   12 root     root          4096 Jul 27 02:35 hack
drwxr-xr-x    2 root     root          4096 Jul 27 02:35 logo
drwxr-xr-x   33 root     root          4096 Jul 27 02:35 pkg
drwxr-xr-x    3 root     root          4096 Jul 27 02:35 plugin
drwxr-xr-x    4 root     root          4096 Jul 27 02:35 staging
drwxr-xr-x   17 root     root          4096 Jul 27 02:35 test
drwxr-xr-x    6 root     root          4096 Jul 27 02:35 third_party
drwxr-xr-x    4 root     root          4096 Jul 27 02:35 translations
drwxr-xr-x   16 root     root          4096 Jul 27 02:35 vendor
```


> * mynapp-rs-hostpath.yml
```
# vi mynapp-rs-hostpath.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata: 
  name: mynapp-rs-hp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-hp
  template:
    metadata:
      labels:
        app: mynapp-rs-hp
    spec:
      nodeName: kube-node1
      containers:
      - name: web-server
        image: nginx:alpine
        volumeMounts:
        - name: web-content
          mountPath: /usr/share/nginx/html
        ports:
        - containerPort: 80
      volumes:
      - name: web-content
        hostPath:
          type: Directory
          path: /web_contents
```

> * mynapp-svc-hostpath.yml
```
# vi mynapp-svc-hostpath.yml

apiVersion: v1
kind: Service
metadata:
  name: mynapp-svc-hp
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: mynapp-rs-hp
```



#### 정적 볼륨 프로비저닝
     

/exports/nfs-vol       192.168.122.0/24(rw,no_root_squash,sync,no_subtree_check)


> * mynapp-pv-nfs.yml 생성
```
# vi mynapp-pv-nfs.yml

apiVersion: v1
kind: PersistentVolume
metadata: 
  name: mynapp-pv-nfs
spec: 
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /exports/nfs-vol
    server: kube-master1
```


> * mynapp-pvc-nfs.yml 생성
```
# vi mynapp-pvc-nfs.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mynapp-pvc-nfs
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  volumeName: mynapp-pv-nfs
```



> * mynapp-rs-nfs.yml
```
# vi mynapp-rs-nfs.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-nfs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-nfs
  template:
    metadata:
      labels:
        app: mynapp-rs-nfs
    spec:
      containers:
      - name: web-server
        image: nginx:alpine
        volumeMounts:
        - name: web-content
          mountPath: /usr/share/nginx/html
        ports:
        - containerPort: 80
      volumes:
      - name: web-content
        persistentVolumeClaim:
          claimName: mynapp-pvc-nfs
```


> * mynapp-svc-nfs.yml
```
# vi mynapp-svc-nfs.yml

apiVersion: v1
kind: Service
metadata:
  name: mynapp-svc-nfs
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: mynapp-rs-nfs
```


#### 동적 볼륨 프로비저닝
* Ceph 클러스터
```
# git clone --single-branch --branch release-1.3 https://github.com/rook/rook.git
# cd /home/student/rook/cluster/examples/kubernetes/ceph
# kubectl create -f common.yaml
# kubectl create -f operator.yaml        # 설치 기다리기
rook-ceph-operator-5698b8bd78-5gpkt   1/1     Running             0          62s
rook-discover-vk8k9                   1/1     Running             0          6s
rook-discover-ncs5l                   1/1     Running             0          79s
rook-discover-lhptl                   1/1     Running             0          80s
# kubectl create -f cluster.yaml     # work node가 3대 이상    #  kubectl create -f cluster-on-pvc.yml        # 클라우드 환경

kubectl get po -n rook-cephfs
```


* ceph toolbox
kubectl create -f toolbox.yaml

kubectl get po -n rook-ceph



* ceph Block Storage

cd ~/rook/cluster/examples/kubernetes/ceph/csi/rbd
kubectl create -f storageclass.yaml         (production 환경)


* Ceph File Storage
cd /home/student/rook/cluster/examples/kubernetes/ceph
kubectl create -f filesystem.yaml
cd ~/rook/cluster/examples/kubernetes/ceph/csi/cephfs
kubectl create -f storageclass.yaml


* mynapp-pvc-dynamic.yaml
```
# vi mynapp-pvc-dynamic.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mynapp-pvc-dynamic
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: rook-ceph

```
kubectl -f mynapp-pvc-dynamic.yaml

```
vi mysql.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - image: mysql:5.6
          name: mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-password
                  key: password
            - name: MYSQL_DATABASE # 구성할 database명
              value: wordpress
            - name: MYSQL_USER # database에 권한이 있는 user
              value: wordadmin
            - name: MYSQL_ROOT_HOST # 접근 호스트
              value: '%'  
            - name: MYSQL_PASSWORD # database에 권한이 있는 user의 패스워드
              value: dkagh1.
          ports:
            - containerPort: 3306
              name: mysql
          volumeMounts:
            - name: mysql-pv-volume
              mountPath: /var/lib/mysql
      volumes:
        - name: mysql-pv-volume
          persistentVolumeClaim:
            claimName: mysql-pv-claim
```


```
vi mysql-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  storageClassName: rook-ceph-block
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

```
vi mysql-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  type: ClusterIP
  ports:
    - port: 3306
  selector:
    app: mysql 
```

```
vi wordpress.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
        - image: wordpress
          name: wordpress
          env:
          - name: WORDPRESS_DB_HOST
            value: mysql:3306
          - name: WORDPRESS_DB_NAME
            value: wordpress
          - name: WORDPRESS_DB_USER
            value: wordadmin
          - name: WORDPRESS_DB_PASSWORD
            value: dkagh1.
          ports:
            - containerPort: 80
              name: wordpress
          volumeMounts:
            - name: wordpress-pv-volume
              mountPath: /var/www/html
      volumes:
        - name: wordpress-pv-volume
          persistentVolumeClaim:
            claimName: wordpress-pvc-dynamic
```

```
vi wordpress-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pvc-dynamic
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: rook-cephfs
```


```
vi wordpress-service.yaml

apiVersion: v1
kind: Service
metadata:
  labels:
    app: wordpress
  name: wordpress
spec:
  type: LoadBalance
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    app: wordpress
  ```




> * index.js 생성
```
# vi index.js

const http = require('http');
const os = require('os');
const ip = require('ip');
const dns = require('dns');
console.log(Date());
console.log("...Start My Node.js Application...");
var handler = function(request, response){
  console.log(Date());
  console.log("Received Request From " + request.connection.remoteAddress);
  response.writeHead(200);
  response.write("Message: " + process.argv[2] + "\n");
  response.write("Hostname: " + os.hostname() + "\n");
  response.write("Platform: " + os.platform() + "\n");
  response.write("Uptime: " + os.uptime() + "\n");
  response.write("IP: " + ip.address() + "\n");
  response.write("dns: " + dns.getServers() + "\n");
  response.end();
};
var www = http.createServer(handler);
www.listen(process.env.NODE_PORT);
```



> * Dockerfile 생성
```
vi Dockerfile

FROM node:slim
WORKDIR /usr/src/app
COPY index.js .
RUN npm install ip
ENTRYPOINT ["node", "index.js"]
CMD ["Hello World!"]
EXPOSE 8080-8090/tcp

```

> * mynapp-pod-env.yml
```
# vi mynapp-pod-env.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-env
spec:
  containers:
  - image: union1991/myweb:customport
    name: mynapp
    env:
    - name: NODE_PORT
      value: "8088"
    ports:
    - containerPort: 8088
      protocol: TCP
```

> *
```
# kubectl port-forward mynapp-pod-env 8088:8088

새로운 터미널 켜서
# curl http://localhost:8088
```



#### 컨피그 맵

> *
```
# kubectl create configmap my-config1 --from-literal=key1=value1
# echo value2 > key2
# kubectl create configmap my-config2 --from-file=key2
# echo value3 > key3
# kubectl create configmap my-config3 --from-file=key3=key3.conf
```

> * configmap.yaml 생성
```
# vi configmap.yaml

apiVersion:v1
kind: ConfigMap
metadata:
  name: my-config4
data:
  key4: value4
```

> *
```
# mkdir configmap
# echo "8080" > configmap/nodeport
# echo "Hello World by ConfigMap" > configmap/message
# kubectl create configmap mynapp-pod-options --from-file=configmap/
```


> * mynapp-pod-cm.yml
```
# vi mynapp-pod-cm.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-cm
spec:
  containers:
  - image: union1991/myweb:customport
    name: mynapp
    env:
    - name: NODE_PORT
      valueFrom:
        configMapKeyRef:
          name: mynapp-pod-options
          key: nodeport
    - name: MESSAGE
      valueFrom:
        configMapKeyRef:
          name: mynapp-pod-options
          key: message
    args:
    - $(MESSAGE_
    ports:
    - containerPort: 8080
      protocol: TCP
```

> *
```
# kubectl create -f mynapp-pod-cm.yml
# kubectl port-forward mynapp-pod-cm 8080:8080
```


* 컨피그 맵의 볼륨 사용
> *
```
# mkdir conf
```


> * conf/nginx-gzip.conf
```
server {
  listen 80;
  server_name mynapp.example.com;
  gzip on;
  gzip_types text/plain application/xml;
  location / {
   root /usr/share/nginx/html;
   index index.html;
   }
}  
```


> *
```
# kubectl create configmap nginx-gzip-config --from-file=conf/nginx-gzip.conf
```


> * mynapp-pod-cm-compress.yml
```
# vi mynapp-pod-cm-compress.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-cm-compress
spec:
  containers:
  - image: nginx
    name: mynapp-compress
    volumeMounts:
    - name: nginx-compress-config
      mountPath: /etc/nginx/conf.d
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: nginx-compress-config
    configMap:
      name: nginx-gzip-config
```


> *
```
# kubectl create -f mynapp-pod-cm-compress.yml
```


> * 
```
# kubectl port-forward mynapp-pod-cm-compress 8080:80
```

> * 
```
# curl -H "Accept-Encoding: gzip" -l http://localhost:8080
```





### 1.1 시크릿을 이용한 사용자화


> * 
```
# echo -n "admin" > id.txt
# echo -n "P@sswOrd" > pwd.txt
```


> * 
```
# kubectl create secret generic my-secret --from-file=id.txt --from-file=pwd.txt
# kubectl get secret my-secret
```


> *  user-pass.yaml
```
# vi user-pass.yaml

apiVersion: v1
kind: Secret
metadata:
  name: user-pass-yaml
type: Opaque
data:
  username: YWRtaW4=
  password: UEBzc3dPcmQ=
```


> * 
```
# kubectl create -f mynapp-pod.yaml
```


> *
```
# mkdir nginx-tls
# openssl genrsa -out nginx-tls/nginx-tls.key 2048
# openssl req -new -x509 -key nginx-tls/nginx-tls.key\
-out nginx-tls/nginx-tls.crt \
-days 3650 -subj /CN=mynapp.example.com
# kubectl create secret tls nginx-tls-secret \
--cert=nginx-tls/nginx-tls.crt \
--key=nginx-tls/nginx-tls.key 
```


> * 
```
# mkdir conf
# vi conf/nginx-tls.conf
server {
  listen 80;
  listen 443 ssl;
  server_name mynapp.example.com;
  ssl_certificate /etc/nginx/ssl/tls.crt;
  ssl_certificate_key /etc/nginx/ssl/tls.key;
  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
  ssl_ciphers
  location / {
   root /usr/share/nginx/html;
   index index.html;
   }
}  

```

> *
```
# kubectl create configmap nginx-tls-config --from-file=conf/nginx-tls.conf
```


> * mynapp-pod-https.yml
```
# vi mynapp-pod-https.yml

apiVersion: v1
kind: Pod
metadata:

```


##  디플로이먼트


### .1 레플리케이션 컨트롤러 롤링 업데이트


### .2 디플로이먼트

#### 

> * mynapp-deploy.yml 기본형
```
# vi mynapp-deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mynapp-deploy
  labels:
    app: mynapp-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mynapp-deploy
  template:
    metadata:
      labels:
        app: mynapp-deploy
    spec:
      containers:
      - image: union1991/myweb:v1
        name: mynapp
        ports:
        - containerPort: 8080
```

> * mynapp-deploy-v1.yml rollingupdate 전략 사용
```
# vi mynapp-deploy-v1.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mynapp-deploy
  labels:
    app: mynapp-deploy
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 20
  replicas: 3
  selector:
    matchLabels:
      app: mynapp-deploy
  template:
    metadata:
      labels:
        app: mynapp-deploy
    spec:
      containers:
      - image: union1991/myweb:v1
        name: mynapp
        ports:
        - containerPort: 8080
```
   
> * 
```
# kubectl create -f mynapp-deploy-v1.yml --record
# kubectl create -f mynapp-svc-deploy.yml
```


* 디플로이먼트 롤링업데이트
> * v1 -> v2
```
# kubectl set image deployment mynapp-deploy mynapp=union1991/myweb:v2
```

> * v2 -> v3
```
# kubectl set image deployment mynapp-deploy mynapp=union1991/myweb:v3 --record
```

> * v3 -> v2
```
# kubectl rollout undo deployment mynapp-deploy --to-revision=2
```

> * v2 -> v3
```
# vi mynapp-deploy-v3.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mynapp-deploy
  labels:
    app: mynapp-deploy
   annotations:
     kubernetes.io/change-cause: My Node App version 3
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge:1
  minReadySeconds: 20
  replicas: 3
  selector:
    matchLabels:
      app: mynapp-deploy
  template:
    metadata:
      labels:
        app: mynapp-deploy
    spec:
      containers:
      - image: union1991/myweb:v3
        name: mynapp
        ports:
        - containerPort: 8080
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 8080
```


> * v2 -> v3
```
# kubectl apply -f mynapp-deploy-v3.yml
```



##  오토스케일링


### .1 파드의 리소스 관리


> * mynapp-pod-req.yml 
```
# vi mynapp-pod-req.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-req
spec:
  containers:
  - image: union1991/myweb
    name: mynapp
    resources:
      requests:
        cpu: 200m
        memory: 10Mi
```


> * mynapp-pod-huge-req.yml 
```
# vi mynapp-pod-huge-req.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-huge-req
spec:
  containers:
  - image: union1991/myweb
    name: mynapp
    resources:
      requests:
        cpu: 4000m
        memory: 4Gi
```



* 리소스 제한
> * mynapp-pod-limits.yml
```
# vi mynapp-pod-limits.yml

apiVersion: v1
kind: Pod
metadata:
  name: mynapp-pod-huge-req
spec:
  containers:
  - image: union1991/myweb
    name: mynapp
    resources:
      limits:
        cpu: 0.5     # 50% 이하 사용
        memory: 20Mi

```

> * mynapp-limitrange.yml
```
# vi mynapp-limitrange.yml

apiVerion: v1
kind: LimitRange
metadata:
  name: mynapp-limitrange
spec:
  limits:
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
  - type: container
    defaultRequest:
      cpu: 100m
      memory: 10Mi
    default:
      cpu: 200m
      memory: 100Mi
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim
    min: 
      storage: 10Mi
    max:
      storage: 1Gi
```

> * 
```
# kubectl create -f mynapp-limitrange.yml
# kubectl describe limitranges mynapp-limitrange
```


#### 리소스 쿼터

* CPU 및 메모리 쿼터 
> * mynapp-quota-cpumem.yml
```
# vi mynapp-quota-cpumem.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: mynapp-quota-cpumem
spec:
  hard:
    requests.cpu: 500m
    requests.memory: 200Mi
    limits.cpu: 1000m
    limits.memory: 1Gi
```


* 오브젝트
> * mynapp-quota-object.yml
```
# vi mynapp-quota-object.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: mynapp-quota-object
spec:
  hard:
    pods: 10
    replicateioncontrollers: 2
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 5
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    glusterfile.storageclass.storage.k8s.io/persistentvolumeclaims: 2
```


* 스토리지 쿼터
> * mynapp-quota-storage.yml
```
# vi mynapp-quota-storage.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: mynapp-quota-storage
spec:
  hard:
    requests.storage: 10Gi
    glusterfile.storageclass.storage.k8s.io/requests.storage: 5Gi
```

### .2 오토스케일링 소개

* HPA 디플로이먼트
> * mynapp-deploy-hpa.yml
```
# vi mynapp-deploy-hpa.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mynapp-deploy-hpa
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mynapp-deploy-hpa
  template:
    metadata:
      labels:
        app: mynapp-deploy-hpa
    spec:
      containers:
      - image: union1991/myweb:v1
        name: mynapp
        resources:
          requests:
            cpu: 50m
            memory: 5Mi
          limits:
            cpu: 100m
            memory: 20Mi
        ports:
        - containerPort: 8080
```

* HPA 리소스 생성
> * mynapp-hpa-cpu.yml
> * kubectl autoscale deployment mynapp-deploy-hpa --min 1 --max 5 --cpu-percent 70
```
# vi mynapp-hpa-cpu.yml

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: mynapp-hpa-cpu
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mynapp-deploy-hpa
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
```




##  파드 스케줄러


### .1 노드 네임


> * mynapp-rs-nodename.yml 
```
# vi mynapp-rs-nodename.yml 

apiVersion: apps/v1
kind: Replicaset
metadata:
  name: mynapp-rs-nn
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-nn
  template:
    metadata:
      labels:
        app: mynapp-rs-nn
    spec:
      nodeName: kube-node1
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb
```

> * 확인
```
# kubectl get po -o wide
```

### .2 노드 셀렉터

#### 노드 레이블 설정
> * 
```
# kubectl label nodes kube-node1 gpu=highend
# kubectl label nodes kube-node2 gpu=midrange
# kubectl label nodes kube-node3 gpu=lowend
```

> * mynapp-rs-nodeselect.yml
```
# vi mynapp-rs-nodeselect.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-ns
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-ns
  template:
    metadata:
      labels:
        app: mynapp-rs-ns
    spec:
      nodeSelector:
        gpu: highend
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb
```

> * 확인
```
# kubectl get po -o wide
```


> * 리플리카셋 복제본 개수 확장
```
# kubectl scale rs mynapp-rs-ns --replicas=3
```


> * 확인
```
# kubectl get po -o wide
```


### .3 어피니티


> * mynapp-rs-nodeaffinity.yml
```
# vi mynapp-rs-nodeaffinity.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-nodeaff
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-nodeaff
  template:
    metadata:
      labels:
        app: mynapp-rs-nodeaff
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu
                operator: In
                values:
                - highend
                - midrange
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 10
            preference:
              matchExpressions:
              - key: gpu-model
                operator: In
                values:
                - titan
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb
```
              
> * mynapp-rs-affinity-cache.yml
```
# vi mynapp-rs-affinity-cache.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-aff-cache
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-aff-cache
      tier: cache
  template:
    metadata:
      labels:
        app: mynapp-rs-aff-cache
        tier: cache
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb
```
  
  
> * mynapp-rs-affinity-frontend.yml
```
# vi mynapp-rs-affinity-frontend.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-aff-front
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mynapp-rs-aff-front
      tier: frontend
  template:
    metadata:
      labels:
        app: mynapp-rs-aff-front
        tier: frontend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - frontend
            topologyKey: "kubernetes.io/hostname"
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb
```
  

* 캐시 티어 레플리카셋 리소스의 스케일 확장
> * 
```
# kubectl scale rs mynapp-rs-aff-cache --replicas=3
```

* 프런트엔드 티어 레플리카셋 리소스의 스케일 확장
```
# kubectl scale rs mynapp-rs-aff-front --replicas=3
```



### .4 테인트 및 톨러레이션

> *
```
# kubectl taint node kube-node2 env=production:NoSchedule
```


> * mynapp-rs-notoleration.yml
```
# vi mynapp-rs-notoleration.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-notol
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mynapp-rs-notol
      tier: backend
  template:
    metadata:
      labels:
        app: mynapp-rs-notol
        tier: backend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb        
```

> * mynapp-rs-toleration.yml
```
# vi mynapp-rs-toleration.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mynapp-rs-tol
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mynapp-rs-tol
      tier: backend
  template:
    metadata:
      labels:
        app: mynapp-rs-tol
        tier: backend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      tolerations:
      - key: env
        operator: Equal
        value: production
        effect: NoSchedule
      containers:
      - name: mynapp
        image: c1t1d0s7/myweb        
```

> * 
```
# kubectl taint node kube-node2 env:NoSchedule-
```





### .5 커든 및 드레인

> * 
```
# kubectl cordon kube-node1
# kubectl uncordon kube-node1
```


> * 
```
# kubectl drain kube-node1
# kubectl drain kube-node1 --ignore-daemonsets=true --delete-local-data=true      // daemonset 파드를 제외하고 모든 파드 퇴거(Evict)
```


---


## 인증 및 kubeconfig



curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.11/bin/linux/amd64/kubectl


---


## Helm 및 모니터링, 로깅

### .1 Helm 패키지 관리자


Helm은 Helm 차트라는 구성으로 여러 YAML 오브젝트 리소스를 하나의 패키지로 구성, 배포 및 관리를 쉽게할 수 있다.


#### Helm 설치


Helm은 Kubectl 명령과 적절한 Kubeconfig 파일이 있는 호스트에 설치해야 한다.


* 바이너리
> * 각 운영체제의 Helm 바이너리는 https://get.helm.sh/ 에서 받을 수 있음
```
# wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz
# tar xf helm-v3.2.4-linux-amd64.tar.gz 
# cd ./linux-amd64/
# sudo cp ./helm /usr/local/bin/
# helm version
version.BuildInfo{Version:"v3.2.4", GitCommit:"0ad800ef43d3b826f31a5ad8dfbb4fe05d143688", GitTreeState:"clean", GoVersion:"go1.13.12"}
```


#### Helm 사용


* Helm 사용 조건
> 접근 가능한 쿠버네티스 클러스터
> Helm이 설치될 시스템에 Kubectl이 설치되어 있어야 함
> Kubectl은 적절한 kubeconfig(~/.kube/config)파일로 쿠버네티스 클러스터에 접근할 수 있어야 함


* Helm 허브 저장소 검색
> Helm 허브에서 mysql 차트 검색하는 명령
```
# helm search hub mysql
URL                                                     CHART VERSION   APP VERSION     DESCRIPTION                                       
https://hub.helm.sh/charts/appscode/stash-mysql         8.0.14          8.0.14          stash-mysql - MySQL database backup and restore...
https://hub.helm.sh/charts/wso2/mysql-am                3.1.0-3         5.7             A Helm chart for MySQL based deployment of WSO2...
https://hub.helm.sh/charts/wso2/mysql-ob                1.4.0-1         5.7             A Helm chart for WSO2 Open Banking Datasources    
https://hub.helm.sh/charts/wso2/mysql-ei                6.6.0-1         5.7             A Helm chart for WSO2 Enterprise Integrator Dat...
https://hub.helm.sh/charts/wso2/mysql-is                5.10.0-1        5.7             A Helm chart for the deployment of WSO2 IAM dat...
https://hub.helm.sh/charts/banzaicloud-stable/m...      0.1.0           0.2.0           A Helm chart for deploying the Oracle MySQL Ope...
https://hub.helm.sh/charts/banzaicloud-stable/p...      0.2.4           v0.11.0         A Helm chart for prometheus mysql exporter with...
https://hub.helm.sh/charts/banzaicloud-stable/tidb      0.0.2                           A TiDB Helm chart for Kubernetes                  
https://hub.helm.sh/charts/choerodon/create-mys...      0.1.0           1.0             A Helm chart for Kubernetes                       
https://hub.helm.sh/charts/choerodon/mysql              0.1.4           0.1.4           mysql for Choerodon                               
https://hub.helm.sh/charts/choerodon/mysqld-exp...      0.1.0           1.0             A Helm chart for Kubernetes                       
https://hub.helm.sh/charts/choerodon/mysql-client       0.1.1           0.1.1           mysql Ver 15.1 Distrib 10.1.32-MariaDB, for Lin...
https://hub.helm.sh/charts/t3n/mysql-backup             2.0.0                                                                             
https://hub.helm.sh/charts/t3n/cloudsql-proxy           2.0.0           1.16            Google Cloud SQL Proxy                            
https://hub.helm.sh/charts/cetic/adminer                0.1.4           4.7.6           Adminer is a full-featured database management ...
...

```


> Helm repo 에서 mysql 찯트 검색하는 명령
```
# helm search repo mysql
NAME                                    CHART VERSION   APP VERSION     DESCRIPTION                                       
stable/mysql                            1.6.6           5.7.30          Fast, reliable, scalable, and easy to use open-...
stable/mysqldump                        2.6.0           2.4.1           A Helm chart to help backup MySQL databases usi...
stable/prometheus-mysql-exporter        0.7.0           v0.11.0         A Helm chart for prometheus mysql exporter with...
stable/percona                          1.2.1           5.7.26          free, fully compatible, enhanced, open source d...
stable/percona-xtradb-cluster           1.0.5           5.7.19          free, fully compatible, enhanced, open source d...
stable/phpmyadmin                       4.3.5           5.0.1           DEPRECATED phpMyAdmin is an mysql administratio...
stable/gcloud-sqlproxy                  0.6.1           1.11            DEPRECATED Google Cloud SQL Proxy                 
stable/mariadb                          7.3.14          10.3.22         DEPRECATED Fast, reliable, scalable, and easy t...
```



* Helm 차트 저장소 추가/제거
> 구글에서 제공하는 저장소 추가
```
# helm repo add stable https://kubernetes-charts.storage.googleapis.com
"stable" has been added to your repositories
# helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈ Happy Helming!⎈ 
# helm repo list
NAME    URL                                             
stable  https://kubernetes-charts.storage.googleapis.com
```


> 저장소 제거
```
# helm repo remove stable
"stable" has been removed from your repositoies
```



* 차트 정보 확인
> stable/mysql 차트 확인
```
# helm show chart stable/mysql
apiVersion: v1
appVersion: 5.7.30
description: Fast, reliable, scalable, and easy to use open-source relational database
  system.
home: https://www.mysql.com/
icon: https://www.mysql.com/common/logos/logo-mysql-170x115.png
keywords:
- mysql
- database
- sql
maintainers:
- email: o.with@sportradar.com
  name: olemarkus
- email: viglesias@google.com
  name: viglesiasce
name: mysql
sources:
- https://github.com/kubernetes/charts
- https://github.com/docker-library/mysql
version: 1.6.6
```


> values(변수 정보 확인)
```
# helm show values stable/mysql

## mysql image version
## ref: https://hub.docker.com/r/library/mysql/tags/
##
image: "mysql"
imageTag: "5.7.30"

...

## Init container resources defaults
initContainer:
  resources:
    requests:
      memory: 10Mi
      cpu: 10m
```


* Helm 차트 기본 설치
> mydb 릴리즈 이름으로 stable 저장소의 mysql 설치
```
# helm install mydb stable/mysql
NAME: mydb
LAST DEPLOYED: Mon Aug  3 02:10:48 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
mydb-mysql.default.svc.cluster.local

To get your root password run:

    MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mydb-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, then provide your password:
    $ mysql -h mydb-mysql -p

To connect to your database directly from outside the K8s cluster:
    MYSQL_HOST=127.0.0.1
    MYSQL_PORT=3306

    # Execute the following command to route the connection:
    kubectl port-forward svc/mydb-mysql 3306

    mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}

# kubectl get secret --namespace default mydb-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo
RkCOgXko86
```


* Helm 차트의 root 패스워드 구성 수정 및 배포
> yaml 파일을 이용한 values 옵션
```
# echo "mysqlRootPassword: P@ssw0rd" > custom.yaml
# helm install mydb-custom stable/mysql --values custom.yaml
# kubectl get secret --namespace default mydb-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo
P@ssw0rd
```


> 차트 내용 수정
```
# helm upgrade mydb-custom stable/mysql --set service.port=3306
```


> 차트 내용 복원
```
# helm rollback mydb-custom 1
```


### .2 Prometheus 모니터링

#### 쿠버네티스 기본 모니터링 개요


* Prometheus 아키텍처


* Prometheus 구성 요소
> Prometheus 서버: 시계열 데이터를 취득하고 저장
> pushgateway: Job 리소스와 같은 생명주기가 짧은 리소스의 메트릭 수집
> 크라이언트 라이브러리: 메트릭 수집
> Exporters: HAProxy, StatsD, Graphite와 같은 서비스의 메트릭 내보내기(Prometheus로 가져오기)
> Alertmanager: 알람 전송
> Gragana: PromQL을 이용하여 Prometheus 서버로 부터 데이터를 가져와서 시각화 제공
> Service Discovery: 메트릭 측정 대상 찾기


#### Prometheus Operator 란

* Operator란
> Operator Framwork는여러 리소스를 개별적으로 관리하던 것을 효과적이고 자동화되며 확장 가능한 방식으로 개발하고 관리하기 위한 오픈소스 툴킷

* Prometheus Operator용 네임스페이스 생성
> Prometheus Operator를 설치하기 위한 monitoring 네임스페이스 생성
```
# kubectl create namespace monitoring
# vi prometheus-values.yaml
grafana:
  service:
    type: NodePort
prometheus:
  service:
    type: LoadBalancer
    
# helm install monitor stable/prometheus-operator -f prometheus-values.yaml -n monitoring
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
NAME: monitor
LAST DEPLOYED: Mon Aug  3 05:05:21 2020
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
The Prometheus Operator has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=monitor"

Visit https://github.com/coreos/prometheus-operator for instructions on how
to create & configure Alertmanager and Prometheus instances using the Operator.
```

* Grafana 대시보드 확인
> 관리자 계정: admin
> 패스워드 : prom-operator


![Screenshot from 2020-08-03 14-11-27](https://user-images.githubusercontent.com/56064985/89147968-4b646180-d593-11ea-81b7-7fe364faa2e7.png)




### .3 Elastic Stack 로깅

#### 쿠버네티스 기본 로깅 개요

* Helm을 이용한 Elastic Stack 설치
> Elastic 저장소 추가
```
# helm repo add elastic https://helm.elastic.co
# helm repo update
```

> Elastic Stack용 네임스페이스 생성
```
# kubectl create namespace logging
```


> Elasticsearch 설치
```
# vi elastic-value.yaml

esJavaOpts: "-Xmx128m -Xmx128m"
resources:
  requests:
    cpu: "100m"
    memory: "256M"
  limits:
    cpu: "1000m"
    memory: "1024M"

# helm install search elastic/elasticsearch -f elastic-value.yaml -n logging
# kubectl get all,pv,pvc -n logging


```



